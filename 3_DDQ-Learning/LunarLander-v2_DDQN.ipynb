{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ec9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Deep Q Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,learning_rate,input_dims,fc1_dims,fc2_dims,num_actions):\n",
    "        super(DQN,self).__init__()\n",
    "        \n",
    "        print(\"input dimensions: \",input_dims[0],\"number of actions: \",num_actions)\n",
    "        \n",
    "        # Three fully connected layers\n",
    "        self.fc1 = nn.Linear(*input_dims,fc1_dims) # * unpacks the list of input_dims since it's 2-dimensional (same as input_dims[0])\n",
    "        self.fc2 = nn.Linear(fc1_dims,fc2_dims)\n",
    "        self.fc3 = nn.Linear(fc2_dims,num_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu:0')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Agent \n",
    "class Agent:\n",
    "    def __init__(self,discount,epsilon,learning_rate,input_dims,batch_size,num_actions,mem_size=500000,min_epsilon=0.05,epsilon_decay=0.0005):\n",
    "        # Brain of the agent\n",
    "        self.DQN = DQN(learning_rate,input_dims,256,256,num_actions)\n",
    "        self.DQN_next = DQN(learning_rate,input_dims,256,256,num_actions)\n",
    "        self.DQN_next.load_state_dict(self.DQN.state_dict()) # loads a model's parameter dictionary using a deserialized state_dict\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.action_space = [i for i in range(num_actions)]\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        # Initialize memory \n",
    "        self.state_mem = np.zeros((mem_size,*input_dims),dtype=np.float32) # mem_size number of dim[0]*dim[1] matrices of zeros\n",
    "        self.new_state_mem = np.zeros((mem_size,*input_dims),dtype=np.float32)\n",
    "        self.action_mem = np.zeros(mem_size,dtype=np.int32)\n",
    "        self.reward_mem = np.zeros(mem_size,dtype=np.float32)\n",
    "        self.terminal_mem = np.zeros(mem_size,dtype=bool) # boolean value indicating whether it's the last memory\n",
    "        \n",
    "    # Store records inside the memory\n",
    "    def storage(self,state,new_state,action,reward,terminal):\n",
    "        index = self.mem_counter%self.mem_size # equal to mem_counter if mem_counter < mem_size\n",
    "        self.state_mem[index] = state\n",
    "        self.new_state_mem[index] = new_state\n",
    "        self.reward_mem[index] = reward\n",
    "        self.action_mem[index] = action\n",
    "        self.terminal_mem[index] = terminal\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    # Take actions\n",
    "    def get_action(self,observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Randomly choose one action from the action space\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = T.tensor([observation]).to(self.DQN.device)\n",
    "            actions = self.DQN(state.float()) # same as self.to(torch.float32)\n",
    "            action = T.argmax(actions).item() # the action corresponding to the index of the maximizing action in the actions tensor\n",
    "        return action\n",
    "    \n",
    "    # Learning\n",
    "    def learn(self):\n",
    "        # Check memory counter against batch size since the latter comes from the former\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.DQN.optimizer.zero_grad()\n",
    "        \n",
    "        # Generate batch such that the batch size is smaller than the number of results in the memory\n",
    "        max_batch = min(self.mem_counter,self.mem_size)\n",
    "        batch = np.random.choice(max_batch,self.batch_size,replace=False)\n",
    "        batch_index = np.arange(self.batch_size,dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_mem[batch]).to(self.DQN.device)\n",
    "        new_state_batch = T.tensor(self.new_state_mem[batch]).to(self.DQN.device)\n",
    "        reward_batch = T.tensor(self.reward_mem[batch]).to(self.DQN.device)\n",
    "        terminal_batch = T.tensor(self.terminal_mem[batch]).to(self.DQN.device)\n",
    "        action_batch = self.action_mem[batch] # cannot follow the above format since used as index\n",
    "        \n",
    "        q_next = self.DQN_next(new_state_batch)\n",
    "        q_eval = self.DQN(new_state_batch)\n",
    "        best_action = T.argmax(q_eval,dim=1).int().cpu().numpy()\n",
    "        \n",
    "        q_target = reward_batch+self.discount*q_next[batch_index,best_action]\n",
    "        q_pred = self.DQN(state_batch)[batch_index,action_batch]\n",
    "        q_target[terminal_batch] = 0.0\n",
    "        \n",
    "        # Backprop \n",
    "        loss = self.DQN.loss(q_target,q_pred).to(self.DQN.device)\n",
    "        loss.backward() # get the gradient of the current tensor\n",
    "        self.DQN.optimizer.step()\n",
    "        self.update_epsilon()\n",
    "        \n",
    "    # Epsilon decay\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon = self.epsilon-self.epsilon_decay\n",
    "    \n",
    "    def update_network(self):\n",
    "        self.DQN_next.load_state_dict(self.DQN.state_dict())      \n",
    "\n",
    "        \n",
    "# Model training\n",
    "if __name__=='__main__':\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    input_dims = [env.observation_space.shape[0]]\n",
    "    num_actions = env.action_space.n\n",
    "    discount = 0.99\n",
    "    epsilon = 1.0\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    \n",
    "    agent = Agent(discount,epsilon,learning_rate,input_dims,batch_size,num_actions)\n",
    "    \n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    episodes = 500\n",
    "    learn = True\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            new_state,reward,done,info = env.step(action)\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                env.render()\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.storage(state,new_state,action,reward,done)\n",
    "            if learn:\n",
    "                agent.learn()\n",
    "                # Update target every 5 episodes\n",
    "                if i%5 == 0  and i != 0:\n",
    "                    agent.update_network()\n",
    "                    \n",
    "            state = new_state\n",
    "            \n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-50:])\n",
    "        avg_scores.append(avg_score)\n",
    "        print(\"Episode: \",i,\"\\tScore: \",score,\"\\tAverage Score: %.3f\"% avg_score,\"Epsilon %.3f\" % agent.epsilon)\n",
    "        \n",
    "        # Early stopping when average score of the last 10 episodes is above 200\n",
    "        if np.mean(scores[-50:]) > 200:\n",
    "            episodes = i+1\n",
    "            break\n",
    "    \n",
    "    checkpoint = {\"model\":agent.DQN.state_dict(),\"score\":scores,\"episodes\":episodes} \n",
    "    #save_dir = \"C:/Users/Yang Yue/OneDrive/Documents/GitHub/STAT_430/prep/\"\n",
    "    save_dir = \"/Users/Yang/OneDrive/Documents/GitHub/STAT_430/prep/\"\n",
    "    T.save(checkpoint,save_dir+\"lunarlander-DDQN.pt\")\n",
    "    \n",
    "    x = [i+1 for i in range(episodes)]\n",
    "    plt.plot(x,scores,label=\"episode_reward\")\n",
    "    plt.plot(x,avg_scores,label=\"average_reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Learning Curve DDQN\")\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd752a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2e443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0a0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916c815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
